{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Created by Berkay Alan**\n",
    "\n",
    "**Natural Language Proccessing Series 3 - Object Standardization and Linguistic Features**\n",
    "\n",
    "**25 of February, 2022**\n",
    "\n",
    "For more Tutorial: https://www.kaggle.com/berkayalan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Object Standardization\n",
    "\n",
    "- Linguistic Features : N-Gram\n",
    "\n",
    "- Linguistic Features : Part of speech tagging (POS)\n",
    "\n",
    "- Linguistic Features : Chunking(Shallow Parsing)\n",
    "\n",
    "- Linguistic Features : Noun Chunks\n",
    "\n",
    "- Linguistic Features : Named Entity Recognition(NER)\n",
    "\n",
    "- Linguistic Features : Visualization in Spacy\n",
    "\n",
    "***\n",
    "\n",
    "For the string essentials, please check the first part [here](https://github.com/berkayalan/Data-Science-Tutorials/blob/master/Natural%20Language%20Processing/Natural%20Language%20Proccessing%20Series%201-%20String%20Essentials.ipynb).\n",
    "\n",
    "For the text preprocessing, please check the second part [here](https://github.com/berkayalan/Data-Science-Tutorials/blob/master/Natural%20Language%20Processing/Natural%20Language%20Proccessing%20Series%202-%20Text%20Preprocessing.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [**What Is Natural Language Processing?**](https://machinelearningmastery.com/natural-language-processing/)\n",
    "\n",
    "- [**Text Processing in Python**](https://towardsdatascience.com/text-processing-in-python-29e86ea4114c)\n",
    "\n",
    "- [**Ultimate Guide to Understand and Implement Natural Language Processing (with codes in Python)**](https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/)\n",
    "\n",
    "- [**Natural Language Processing (NLP) with BERT**](https://www.udemy.com/course/natural-language-processing-with-bert/learn/lecture/18889316?start=0#overview)\n",
    "\n",
    "- [**How to Clean Text for Machine Learning with Python**](https://machinelearningmastery.com/clean-text-machine-learning-python/)\n",
    "\n",
    "- [**Dropping common terms: stop words by StanfordNLP**](https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html)\n",
    "\n",
    "- [**Natural Language Processing With Python's NLTK Package**](https://realpython.com/nltk-nlp-python/)\n",
    "\n",
    "- [**What Are n-grams and How to Implement Them in Python?**](https://www.analyticsvidhya.com/blog/2021/09/what-are-n-grams-and-how-to-implement-them-in-python/#:~:text=N%2Dgrams%20are%20continuous%20sequences,(Natural%20Language%20Processing)%20tasks.)\n",
    "\n",
    "- [**N-Gram Language Modelling with NLTK**](https://www.geeksforgeeks.org/n-gram-language-modelling-with-nltk/)\n",
    "\n",
    "- [**Neural Network Embeddings Explained**](https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526)\n",
    "\n",
    "- [**Linguistic Features of Spacy**](https://spacy.io/usage/linguistic-features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download(\"punkt\")\n",
    "#nltk.download(\"wordnet\")\n",
    "#nltk.download(\"averaged_perceptron_tagger\")\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from textblob import TextBlob,Word\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from textblob import Word,TextBlob\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy import displacy\n",
    "import en_core_web_sm\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to load *en_core_web_sm* for **spacy** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see all rows and columns, we will increase max display numbers of dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Text data often contains words or phrases which are not present in any standard lexical dictionaries. These pieces are not recognized by search engines and models.\n",
    "\n",
    "Some of the examples are – acronyms, hashtags with attached words, and colloquial slangs. With the help of regular expressions and manually prepared data dictionaries, this type of noise can be fixed, the code below uses a dictionary lookup method to replace social media slangs from a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbr_dict = {'rt':'Retweet', 'dm':'direct message', \"awsm\" : \"awesome\", \"luv\" :\"love\", \"gg\":\"good game\",\n",
    "               'fb': 'Facebook' , 'ig': 'Instagram','li': 'LinkedIn','sc': 'SnapChat', 'tw': 'Twitter',\n",
    "               'yt': 'YouTube', 'ds':'data science'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_text(input_text):\n",
    "    words = input_text.split() \n",
    "    new_words = [] \n",
    "    for word in words:\n",
    "        if word.lower() in abbr_dict:\n",
    "            word = abbr_dict[word.lower()]\n",
    "        new_words.append(word) \n",
    "        new_text = \" \".join(new_words) \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Did you see the new post of Andrew NG on LinkedIn ? He was sad about Elon Musk's Retweet on Twitter saying data science is a hype.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardize_text(\"\"\"Did you see the new post of Andrew NG on li ?\n",
    "                 He was sad about Elon Musk's rt on TW saying ds is a hype.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistic Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Processing raw text intelligently is difficult: most words are rare, and it’s common for words that look completely different to mean almost the same thing. The same words in a different order can mean something completely different. Even splitting text into useful word-like units can be difficult in many languages. While it’s possible to solve some problems starting from only the raw characters, it’s usually better to use linguistic knowledge to add useful information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A combination of N items together are called N-Grams. The items can be letters, words, or base pairs according to the application. The N-grams typically are collected from a text or speech corpus (A long text dataset).\n",
    "\n",
    "**N-gram Language Model**:\n",
    "\n",
    "An N-gram language model predicts the probability of a given N-gram within any sequence of words in the language. A good N-gram model can predict the next word in the sentence i.e the value of p(w|h). \n",
    "\n",
    "**What are typical applications of N-gram models?**\n",
    "\n",
    "N grams (N > 1) are generally more informative as compared to words (Unigrams) as features. Also, bigrams (N = 2) are considered as the most important features of all the others.\n",
    "\n",
    "Example of N-gram such as unigram (“This”, “article”, “is”, “on”, “NLP”)  or bi-gram (‘This article’, ‘article is’, ‘is on’,’on NLP’).\n",
    "\n",
    "We will use **ngrams()** function of TextBlob library here. It takes *N* as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Companies are very complex so sometimes it's hard to enforce the law when \n",
    "they are spread out around the world. \n",
    "Can we punish a parent company for something it's responsible for in another country?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Companies', 'are']),\n",
       " WordList(['are', 'very']),\n",
       " WordList(['very', 'complex']),\n",
       " WordList(['complex', 'so']),\n",
       " WordList(['so', 'sometimes']),\n",
       " WordList(['sometimes', 'it']),\n",
       " WordList(['it', \"'s\"]),\n",
       " WordList([\"'s\", 'hard']),\n",
       " WordList(['hard', 'to']),\n",
       " WordList(['to', 'enforce']),\n",
       " WordList(['enforce', 'the']),\n",
       " WordList(['the', 'law']),\n",
       " WordList(['law', 'when']),\n",
       " WordList(['when', 'they']),\n",
       " WordList(['they', 'are']),\n",
       " WordList(['are', 'spread']),\n",
       " WordList(['spread', 'out']),\n",
       " WordList(['out', 'around']),\n",
       " WordList(['around', 'the']),\n",
       " WordList(['the', 'world']),\n",
       " WordList(['world', 'Can']),\n",
       " WordList(['Can', 'we']),\n",
       " WordList(['we', 'punish']),\n",
       " WordList(['punish', 'a']),\n",
       " WordList(['a', 'parent']),\n",
       " WordList(['parent', 'company']),\n",
       " WordList(['company', 'for']),\n",
       " WordList(['for', 'something']),\n",
       " WordList(['something', 'it']),\n",
       " WordList(['it', \"'s\"]),\n",
       " WordList([\"'s\", 'responsible']),\n",
       " WordList(['responsible', 'for']),\n",
       " WordList(['for', 'in']),\n",
       " WordList(['in', 'another']),\n",
       " WordList(['another', 'country'])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(text).ngrams(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Companies', 'are', 'very']),\n",
       " WordList(['are', 'very', 'complex']),\n",
       " WordList(['very', 'complex', 'so']),\n",
       " WordList(['complex', 'so', 'sometimes']),\n",
       " WordList(['so', 'sometimes', 'it']),\n",
       " WordList(['sometimes', 'it', \"'s\"]),\n",
       " WordList(['it', \"'s\", 'hard']),\n",
       " WordList([\"'s\", 'hard', 'to']),\n",
       " WordList(['hard', 'to', 'enforce']),\n",
       " WordList(['to', 'enforce', 'the']),\n",
       " WordList(['enforce', 'the', 'law']),\n",
       " WordList(['the', 'law', 'when']),\n",
       " WordList(['law', 'when', 'they']),\n",
       " WordList(['when', 'they', 'are']),\n",
       " WordList(['they', 'are', 'spread']),\n",
       " WordList(['are', 'spread', 'out']),\n",
       " WordList(['spread', 'out', 'around']),\n",
       " WordList(['out', 'around', 'the']),\n",
       " WordList(['around', 'the', 'world']),\n",
       " WordList(['the', 'world', 'Can']),\n",
       " WordList(['world', 'Can', 'we']),\n",
       " WordList(['Can', 'we', 'punish']),\n",
       " WordList(['we', 'punish', 'a']),\n",
       " WordList(['punish', 'a', 'parent']),\n",
       " WordList(['a', 'parent', 'company']),\n",
       " WordList(['parent', 'company', 'for']),\n",
       " WordList(['company', 'for', 'something']),\n",
       " WordList(['for', 'something', 'it']),\n",
       " WordList(['something', 'it', \"'s\"]),\n",
       " WordList(['it', \"'s\", 'responsible']),\n",
       " WordList([\"'s\", 'responsible', 'for']),\n",
       " WordList(['responsible', 'for', 'in']),\n",
       " WordList(['for', 'in', 'another']),\n",
       " WordList(['in', 'another', 'country'])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(text).ngrams(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do that with sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.Series([\n",
    "    \"The Lion is the king of the jungle\",\n",
    "    \"Lions have lifespans of a decade\",\n",
    "    \"The lion is an endangered species\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate n-grams upto n=1\n",
    "vectorizer_ng1 = CountVectorizer(ngram_range=(1,1))\n",
    "ng1 = vectorizer_ng1.fit_transform(corpus)\n",
    "\n",
    "# Generate n-grams upto n=2\n",
    "vectorizer_ng2 = CountVectorizer(ngram_range=(1,2))\n",
    "ng2 = vectorizer_ng2.fit_transform(corpus)\n",
    "\n",
    "# Generate n-grams upto n=3\n",
    "vectorizer_ng3 = CountVectorizer(ngram_range=(1, 3))\n",
    "ng3 = vectorizer_ng3.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 12)\t3\n",
      "  (0, 8)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 5)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 1)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 11)\t1\n"
     ]
    }
   ],
   "source": [
    "print(ng1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng1, ng2 and ng3 have 13, 27 and 39 features respectively\n"
     ]
    }
   ],
   "source": [
    "# Print the number of features for each model\n",
    "print(\"ng1, ng2 and ng3 have %i, %i and %i features respectively\" % (ng1.shape[1], ng2.shape[1], ng3.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech tagging (POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of speech is a grammatical term that deals with the roles words play when we use them together in sentences. Tagging parts of speech, or POS tagging, is the task of labeling the words in our text according to their part of speech.\n",
    "\n",
    " We can get parts of speech from both ```Textblob.tag``` and ```spacy.pos_```. All tags can be seen as below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Abbrv| Details |\n",
    "| --- | --- |\n",
    "| CC | coordinating conjunction |\n",
    "| CD | cardinal digit |\n",
    "| DT | determiner |\n",
    "| EX | existential there (like: “there is” … think of it like “there exists”) |\n",
    "| FW | foreign word |\n",
    "| IN | preposition/subordinating conjunction |\n",
    "| JJ | adjective ‘big’ |\n",
    "| JJR | adjective, comparative ‘bigger’  |\n",
    "| JJS | adjective, superlative ‘biggest’  |\n",
    "| LS | list marker 1) |\n",
    "| MD | modal could, will |\n",
    "| NN | noun, singular ‘desk’ |\n",
    "| NNS | noun plural ‘desks’  |\n",
    "| NNP | proper noun, singular ‘Harrison’  |\n",
    "| NNPS | proper noun, plural ‘Americans’ |\n",
    "| PDT | predeterminer ‘all the kids’  |\n",
    "| POS | possessive ending parent‘s  |\n",
    "| PRP | personal pronoun I, he, she  |\n",
    "| PRP(sign) | possessive pronoun my, his, hers |\n",
    "| RB | adverb very, silently |\n",
    "| RBR | adverb, comparative better  |\n",
    "| RBS | adverb, superlative best  |\n",
    "| RP |  particle give up |\n",
    "| TO |  to go ‘to‘ the store. |\n",
    "| UH |  interjection errrrrrrrm |\n",
    "| VB |  verb, base form take |\n",
    "| VBD | verb, past tense took  |\n",
    "| VBG | verb, gerund/present participle taking  |\n",
    "| VBN | verb, past participle taken  |\n",
    "| VBP | verb, sing. present, non-3d take  |\n",
    "| VBZ | verb, 3rd person sing. present takes  |\n",
    "| WDT | wh-determiner which  |\n",
    "| WP |  wh-pronoun who, what |\n",
    "| WP$ | possessive wh-pronoun whose  |\n",
    "| WRB | wh-abverb where, when  |\n",
    "| ADJ | adjective | \n",
    "| ADP | adposition | \n",
    "| ADV | adverb | \n",
    "| AUX | auxiliary | \n",
    "| CONJ | conjunction | \n",
    "| CCONJ | coordinating | \n",
    "| DET | determiner | \n",
    "| INTJ | interjection | \n",
    "| NOUN | noun | \n",
    "| NUM | numeral | \n",
    "| PART | particle | \n",
    "| PRON | pronoun | \n",
    "| PROPN | proper | \n",
    "| PUNCT | punctuation | \n",
    "| SCONJ | subordinating | \n",
    "| SYM | symbol | \n",
    "| VERB | verb | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a current list of tags for all languages visit https://spacy.io/api/annotation#pos-tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make an example and assign every words is corresponding part of speech.\n",
    "\n",
    "*Berkay is an amazing football player*.\n",
    "\n",
    "**POS Tagging**\n",
    "\n",
    "Berkay -> prober noun\n",
    "\n",
    "is -> verb\n",
    "\n",
    "an -> determiner\n",
    "\n",
    "amazing -> adjective\n",
    "\n",
    "football -> noun\n",
    "\n",
    "player -> noun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get started with *Textblob*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"The titular threat of The Blob has always struck me as the ultimate movie\n",
    "monster: an insatiably hungry, amoeba-like mass able to penetrate\n",
    "virtually any safeguard, capable of--as a doomed doctor chillingly\n",
    "describes it--\"assimilating flesh on contact.\n",
    "Snide comparisons to gelatin be damned, it's a concept with the most\n",
    "devastating of potential consequences, not unlike the grey goo scenario\n",
    "proposed by technological theorists fearful of\n",
    "artificial intelligence run rampant.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('titular', 'JJ'),\n",
       " ('threat', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('The', 'DT'),\n",
       " ('Blob', 'NNP'),\n",
       " ('has', 'VBZ'),\n",
       " ('always', 'RB'),\n",
       " ('struck', 'VBN'),\n",
       " ('me', 'PRP'),\n",
       " ('as', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('ultimate', 'JJ'),\n",
       " ('movie', 'NN'),\n",
       " ('monster', 'NN'),\n",
       " ('an', 'DT'),\n",
       " ('insatiably', 'RB'),\n",
       " ('hungry', 'JJ'),\n",
       " ('amoeba-like', 'JJ'),\n",
       " ('mass', 'NN'),\n",
       " ('able', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('penetrate', 'VB'),\n",
       " ('virtually', 'RB'),\n",
       " ('any', 'DT'),\n",
       " ('safeguard', 'NN'),\n",
       " ('capable', 'JJ'),\n",
       " ('of', 'IN'),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('doomed', 'JJ'),\n",
       " ('doctor', 'NN'),\n",
       " ('chillingly', 'RB'),\n",
       " ('describes', 'VBZ'),\n",
       " ('it', 'PRP'),\n",
       " ('assimilating', 'VBG'),\n",
       " ('flesh', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('contact', 'NN'),\n",
       " ('Snide', 'JJ'),\n",
       " ('comparisons', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('gelatin', 'VB'),\n",
       " ('be', 'VB'),\n",
       " ('damned', 'VBN'),\n",
       " ('it', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('concept', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('most', 'RBS'),\n",
       " ('devastating', 'JJ'),\n",
       " ('of', 'IN'),\n",
       " ('potential', 'JJ'),\n",
       " ('consequences', 'NNS'),\n",
       " ('not', 'RB'),\n",
       " ('unlike', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('grey', 'NN'),\n",
       " ('goo', 'NN'),\n",
       " ('scenario', 'NN'),\n",
       " ('proposed', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('technological', 'JJ'),\n",
       " ('theorists', 'NNS'),\n",
       " ('fearful', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('artificial', 'JJ'),\n",
       " ('intelligence', 'NN'),\n",
       " ('run', 'NN'),\n",
       " ('rampant', 'NN')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(text).tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Companies are very complex so sometimes it's hard to enforce the law when \n",
    "they are spread out around the world. \n",
    "Can we punish a parent company for something it's responsible for in another country?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Companies', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('very', 'RB'),\n",
       " ('complex', 'JJ'),\n",
       " ('so', 'RB'),\n",
       " ('sometimes', 'VBZ'),\n",
       " ('it', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('hard', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('enforce', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('law', 'NN'),\n",
       " ('when', 'WRB'),\n",
       " ('they', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('spread', 'VBN'),\n",
       " ('out', 'RP'),\n",
       " ('around', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('world', 'NN'),\n",
       " ('Can', 'MD'),\n",
       " ('we', 'PRP'),\n",
       " ('punish', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('parent', 'NN'),\n",
       " ('company', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('it', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('responsible', 'JJ'),\n",
       " ('for', 'IN'),\n",
       " ('in', 'IN'),\n",
       " ('another', 'DT'),\n",
       " ('country', 'NN')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(text).pos_tags # pos_tags is also possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make it with **spacy** library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also obtain a particular token by its index position.\n",
    "\n",
    "* To view the coarse POS tag, we can use `token.pos_`\n",
    "* To view the fine-grained tag, we can use `token.tag_`\n",
    "* To view the description of either type of tag, we can use `spacy.explain(tag)`\n",
    "\n",
    "Note that `token.pos` and `token.tag` return integer hash values; by adding the underscores we get the text equivalent that lives in **doc.vocab**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Companies are very complex so sometimes it's hard to enforce the law when \n",
    "they are spread out around the world. \n",
    "Can we punish a parent company for something it's responsible for in another country?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Companies are very complex so sometimes it's hard to enforce the law when \\nthey are spread out around the world. \\nCan we punish a parent company for something it's responsible for in another country?\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'complex'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ADJ'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3].pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'JJ'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3].tag_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adjective (English), other noun-modifier (Chinese)'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(doc[3].tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In the English language, the same string of characters can have different meanings, even within the same sentence. For this reason, morphology is important. spaCy uses machine learning algorithms to best predict the use of a token in a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply this technique to the entire Doc object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Companies  NOUN     NNS    noun, plural\n",
      "are        AUX      VBP    verb, non-3rd person singular present\n",
      "very       ADV      RB     adverb\n",
      "complex    ADJ      JJ     adjective (English), other noun-modifier (Chinese)\n",
      "so         ADV      RB     adverb\n",
      "sometimes  ADV      RB     adverb\n",
      "it         PRON     PRP    pronoun, personal\n",
      "'s         AUX      VBZ    verb, 3rd person singular present\n",
      "hard       ADJ      JJ     adjective (English), other noun-modifier (Chinese)\n",
      "to         PART     TO     infinitival \"to\"\n",
      "enforce    VERB     VB     verb, base form\n",
      "the        DET      DT     determiner\n",
      "law        NOUN     NN     noun, singular or mass\n",
      "when       SCONJ    WRB    wh-adverb\n",
      "\n",
      "          SPACE    _SP    whitespace\n",
      "they       PRON     PRP    pronoun, personal\n",
      "are        AUX      VBP    verb, non-3rd person singular present\n",
      "spread     VERB     VBN    verb, past participle\n",
      "out        ADP      RP     adverb, particle\n",
      "around     ADP      IN     conjunction, subordinating or preposition\n",
      "the        DET      DT     determiner\n",
      "world      NOUN     NN     noun, singular or mass\n",
      ".          PUNCT    .      punctuation mark, sentence closer\n",
      "\n",
      "          SPACE    _SP    whitespace\n",
      "Can        AUX      MD     verb, modal auxiliary\n",
      "we         PRON     PRP    pronoun, personal\n",
      "punish     VERB     VB     verb, base form\n",
      "a          DET      DT     determiner\n",
      "parent     NOUN     NN     noun, singular or mass\n",
      "company    NOUN     NN     noun, singular or mass\n",
      "for        ADP      IN     conjunction, subordinating or preposition\n",
      "something  PRON     NN     noun, singular or mass\n",
      "it         PRON     PRP    pronoun, personal\n",
      "'s         AUX      VBZ    verb, 3rd person singular present\n",
      "responsible ADJ      JJ     adjective (English), other noun-modifier (Chinese)\n",
      "for        ADP      IN     conjunction, subordinating or preposition\n",
      "in         ADP      IN     conjunction, subordinating or preposition\n",
      "another    DET      DT     determiner\n",
      "country    NOUN     NN     noun, singular or mass\n",
      "?          PUNCT    .      punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f'{token.text:{10}} {token.pos_:{8}} {token.tag_:{6}} {spacy.explain(token.tag_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that returns number of proper nouns in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proper_nouns(text, model=nlp):\n",
    "    # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of proper nouns\n",
    "    return pos.count(\"PROPN\") # NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(proper_nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Doc.count_by()` method accepts a specific token attribute as its argument, and returns a frequency count of the given attribute as a dictionary object. Keys in the dictionary are the integer values of the given attribute ID, and values are the frequency. Counts of zero are not included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Companies are very complex so sometimes it's hard to enforce the law when \\nthey are spread out around the world. \\nCan we punish a parent company for something it's responsible for in another country?\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the frequencies of different coarse-grained POS tags. It returns a dictionary with vocab indexes and counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{92: 6,\n",
       " 87: 5,\n",
       " 86: 3,\n",
       " 84: 3,\n",
       " 95: 5,\n",
       " 94: 1,\n",
       " 100: 3,\n",
       " 90: 4,\n",
       " 98: 1,\n",
       " 103: 2,\n",
       " 85: 5,\n",
       " 97: 2}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.count_by(spacy.attrs.POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOUN'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.vocab[92].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84. ADJ     3\n",
      "85. ADP     5\n",
      "86. ADV     3\n",
      "87. AUX     5\n",
      "90. DET     4\n",
      "92. NOUN    6\n",
      "94. PART    1\n",
      "95. PRON    5\n",
      "97. PUNCT   2\n",
      "98. SCONJ   1\n",
      "100. VERB    3\n",
      "103. SPACE   2\n"
     ]
    }
   ],
   "source": [
    "for key,value in sorted(doc.count_by(spacy.attrs.POS).items()):\n",
    "    print(f\"{key}. {doc.vocab[key].text:{7}} {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking (shallow parsing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking makes use of POS tags to group words and apply chunk tags to those groups. Chunks don’t overlap, so one instance of a word can be in only one chunk at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Companies are very complex so sometimes it's hard to enforce the law when \n",
    "they are spread out around the world. \n",
    "Can we punish a parent company for something it's responsible for in another country?\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_tags = TextBlob(text).tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Companies', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('very', 'RB'),\n",
       " ('complex', 'JJ'),\n",
       " ('so', 'RB'),\n",
       " ('sometimes', 'VBZ'),\n",
       " ('it', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('hard', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('enforce', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('law', 'NN'),\n",
       " ('when', 'WRB'),\n",
       " ('they', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('spread', 'VBN'),\n",
       " ('out', 'RP'),\n",
       " ('around', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('world', 'NN'),\n",
       " ('Can', 'MD'),\n",
       " ('we', 'PRP'),\n",
       " ('punish', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('parent', 'NN'),\n",
       " ('company', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('it', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('responsible', 'JJ'),\n",
       " ('for', 'IN'),\n",
       " ('in', 'IN'),\n",
       " ('another', 'DT'),\n",
       " ('country', 'NN')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "rp = nltk.RegexpParser(regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = rp.parse(speech_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Companies/NNS\n",
      "  are/VBP\n",
      "  very/RB\n",
      "  complex/JJ\n",
      "  so/RB\n",
      "  sometimes/VBZ\n",
      "  it/PRP\n",
      "  's/VBZ\n",
      "  hard/JJ\n",
      "  to/TO\n",
      "  enforce/VB\n",
      "  (NP the/DT law/NN)\n",
      "  when/WRB\n",
      "  they/PRP\n",
      "  are/VBP\n",
      "  spread/VBN\n",
      "  out/RP\n",
      "  around/IN\n",
      "  (NP the/DT world/NN)\n",
      "  Can/MD\n",
      "  we/PRP\n",
      "  punish/VB\n",
      "  (NP a/DT parent/NN)\n",
      "  (NP company/NN)\n",
      "  for/IN\n",
      "  (NP something/NN)\n",
      "  it/PRP\n",
      "  's/VBZ\n",
      "  responsible/JJ\n",
      "  for/IN\n",
      "  in/IN\n",
      "  (NP another/DT country/NN))\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will draw the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Doc.ents, **Doc.noun_chunks** are another object property. Noun chunks are \"base noun phrases\" – flat phrases that have a noun as their head. We can think of noun chunks as a noun plus the words describing the noun – for example, “the lavish green grass” or “the world’s largest tech fund”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars\n",
      "insurance liability\n",
      "manufacturers\n"
     ]
    }
   ],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big houses\n",
      "higher insurance rates\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"Big houses carry higher insurance rates.\")\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named entities are noun phrases that refer to specific locations, people, organizations, and so on. With named entity recognition, we can find the named entities in our texts and also determine what kind of named entity they are.\n",
    "\n",
    "We can use **nltk.ne_chunk()** to recognize named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"This page gives an overview of all articles in the 1911 \n",
    "Brittanica which are alphabetized under Nau to Ner.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  This/DT\n",
      "  page/NN\n",
      "  gives/VBZ\n",
      "  an/DT\n",
      "  overview/NN\n",
      "  of/IN\n",
      "  all/DT\n",
      "  articles/NNS\n",
      "  in/IN\n",
      "  the/DT\n",
      "  1911/CD\n",
      "  (GPE Brittanica/NNP)\n",
      "  which/WDT\n",
      "  are/VBP\n",
      "  alphabetized/VBN\n",
      "  under/IN\n",
      "  (PERSON Nau/NNP)\n",
      "  to/TO\n",
      "  (PERSON Ner/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(ne_chunk(pos_tag(word_tokenize(sentence))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ne(quote):\n",
    "    words = word_tokenize(quote, language=\"english\")\n",
    "    tags = nltk.pos_tag(words)\n",
    "    tree = nltk.ne_chunk(tags, binary=True)\n",
    "    return set(\n",
    "        \" \".join(i[0] for i in t)\n",
    "        for t in tree\n",
    "        if hasattr(t, \"label\") and t.label() == \"NE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Nau', 'Ner'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_ne(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets do that with **spacy** library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find named entitites, we need to get label_ from the text object's entities. If we also would like to get details of entities, we can use *spacy.explain* function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tags are accessible through the `.label_` property of an entity.\n",
    "\n",
    "<table>\n",
    "<tr><th>TYPE</th><th>DESCRIPTION</th><th>EXAMPLE</th></tr>\n",
    "<tr><td>`PERSON`</td><td>People, including fictional.</td><td>*Fred Flintstone*</td></tr>\n",
    "<tr><td>`NORP`</td><td>Nationalities or religious or political groups.</td><td>*The Republican Party*</td></tr>\n",
    "<tr><td>`FAC`</td><td>Buildings, airports, highways, bridges, etc.</td><td>*Logan International Airport, The Golden Gate*</td></tr>\n",
    "<tr><td>`ORG`</td><td>Companies, agencies, institutions, etc.</td><td>*Microsoft, FBI, MIT*</td></tr>\n",
    "<tr><td>`GPE`</td><td>Countries, cities, states.</td><td>*France, UAR, Chicago, Idaho*</td></tr>\n",
    "<tr><td>`LOC`</td><td>Non-GPE locations, mountain ranges, bodies of water.</td><td>*Europe, Nile River, Midwest*</td></tr>\n",
    "<tr><td>`PRODUCT`</td><td>Objects, vehicles, foods, etc. (Not services.)</td><td>*Formula 1*</td></tr>\n",
    "<tr><td>`EVENT`</td><td>Named hurricanes, battles, wars, sports events, etc.</td><td>*Olympic Games*</td></tr>\n",
    "<tr><td>`WORK_OF_ART`</td><td>Titles of books, songs, etc.</td><td>*The Mona Lisa*</td></tr>\n",
    "<tr><td>`LAW`</td><td>Named documents made into laws.</td><td>*Roe v. Wade*</td></tr>\n",
    "<tr><td>`LANGUAGE`</td><td>Any named language.</td><td>*English*</td></tr>\n",
    "<tr><td>`DATE`</td><td>Absolute or relative dates or periods.</td><td>*20 July 1969*</td></tr>\n",
    "<tr><td>`TIME`</td><td>Times smaller than a day.</td><td>*Four hours*</td></tr>\n",
    "<tr><td>`PERCENT`</td><td>Percentage, including \"%\".</td><td>*Eighty percent*</td></tr>\n",
    "<tr><td>`MONEY`</td><td>Monetary values, including unit.</td><td>*Twenty Cents*</td></tr>\n",
    "<tr><td>`QUANTITY`</td><td>Measurements, as of weight or distance.</td><td>*Several kilometers, 55kg*</td></tr>\n",
    "<tr><td>`ORDINAL`</td><td>\"first\", \"second\", etc.</td><td>*9th, Ninth*</td></tr>\n",
    "<tr><td>`CARDINAL`</td><td>Numerals that do not fall under another type.</td><td>*2, Two, Fifty-two*</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Doc instance \n",
    "text = 'Huawei to build a Istanbul factory for $4 million.'\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Huawei ORG Companies, agencies, institutions, etc.\n",
      "Istanbul GPE Countries, cities, states\n",
      "$4 million MONEY Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "# Print all named entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_, str(spacy.explain(ent.label_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function to display these info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ents(doc):\n",
    "    if doc.ents:\n",
    "        for ent in doc.ents:\n",
    "            print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))\n",
    "    else:\n",
    "        print('There is no named entities.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Huawei - ORG - Companies, agencies, institutions, etc.\n",
      "Istanbul - GPE - Countries, cities, states\n",
      "$4 million - MONEY - Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "show_ents(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a function to get persons in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = \"\"\"Germany's conservative Christian Democrats have chosen Friedrich Merz,\n",
    "a critic of former Chancellor Angela Merkel, as their new leader in the hope of \n",
    "reviving the party devastated by last year's general election defeat.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Friedrich Merz', 'Angela Merkel']\n"
     ]
    }
   ],
   "source": [
    "def find_persons(text):\n",
    "  # Create Doc object\n",
    "  doc = nlp(text)\n",
    "  \n",
    "  # Identify the persons\n",
    "  persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "  \n",
    "  # Return persons\n",
    "  return persons\n",
    "\n",
    "print(find_persons(news))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all details of name entities in Spacy, please check [here](https://spacy.io/usage/linguistic-features#named-entities)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Adding Named Entities to All Matching Spans**\n",
    "\n",
    "What if we want to tag *all* occurrences of \"Huawei\"? Now we show how to use the PhraseMatcher to identify a series of spans in the Doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Huawei to build a Istanbul factory for $4 million. But their new smartphone are not loved by the audience. Smart-phones are very important for these companies.'\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Huawei - ORG - Companies, agencies, institutions, etc.\n",
      "Istanbul - GPE - Countries, cities, states\n",
      "$4 million - MONEY - Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "show_ents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.matcher.phrasematcher.PhraseMatcher at 0x7ff149b67190>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_list = ['smartphone', 'Smart-phones']\n",
    "phrase_patterns = [nlp(text) for text in phrase_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the patterns to our matcher object:\n",
    "matcher.add('newproduct', None, *phrase_patterns)\n",
    "\n",
    "# Apply the matcher to our Doc object:\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2689272359382549672, 14, 15), (2689272359382549672, 22, 25)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROD = doc.vocab.strings[u'PRODUCT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ents = [Span(doc, match[1],match[2],label=PROD) for match in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.ents = list(doc.ents) + new_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Huawei - ORG - Companies, agencies, institutions, etc.\n",
      "Istanbul - GPE - Countries, cities, states\n",
      "$4 million - MONEY - Monetary values, including unit\n",
      "smartphone - PRODUCT - Objects, vehicles, foods, etc. (not services)\n",
      "Smart-phones - PRODUCT - Objects, vehicles, foods, etc. (not services)\n"
     ]
    }
   ],
   "source": [
    "show_ents(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization in Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " spaCy includes a built-in visualization tool called **displaCy**. displaCy is able to detect whether you're working in a Jupyter notebook, and will return markup that can be rendered in a cell right away. When we export our notebook, the visualizations will be included as HTML.\n",
    " \n",
    " You can check [here](https://spacy.io/usage/visualizers) for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dependency visualizer, *de*p, shows part-of-speech tags and syntactic dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"6f1d22e687504be6aab936e69b8ac3aa-0\" class=\"displacy\" width=\"950\" height=\"317.0\" direction=\"ltr\" style=\"max-width: none; height: 317.0px; color: red; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Huawei</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">build</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">Istanbul</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">factory</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"590\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"590\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"680\">$</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"680\">SYM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">4</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"860\">million.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"860\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f1d22e687504be6aab936e69b8ac3aa-0-0\" stroke-width=\"2px\" d=\"M70,182.0 C70,92.0 220.0,92.0 220.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f1d22e687504be6aab936e69b8ac3aa-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,184.0 L62,172.0 78,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f1d22e687504be6aab936e69b8ac3aa-0-1\" stroke-width=\"2px\" d=\"M160,182.0 C160,137.0 215.0,137.0 215.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f1d22e687504be6aab936e69b8ac3aa-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M160,184.0 L152,172.0 168,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f1d22e687504be6aab936e69b8ac3aa-0-2\" stroke-width=\"2px\" d=\"M340,182.0 C340,92.0 490.0,92.0 490.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f1d22e687504be6aab936e69b8ac3aa-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M340,184.0 L332,172.0 348,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f1d22e687504be6aab936e69b8ac3aa-0-3\" stroke-width=\"2px\" d=\"M430,182.0 C430,137.0 485.0,137.0 485.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f1d22e687504be6aab936e69b8ac3aa-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M430,184.0 L422,172.0 438,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f1d22e687504be6aab936e69b8ac3aa-0-4\" stroke-width=\"2px\" d=\"M250,182.0 C250,47.0 495.0,47.0 495.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f1d22e687504be6aab936e69b8ac3aa-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M495.0,184.0 L503.0,172.0 487.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f1d22e687504be6aab936e69b8ac3aa-0-5\" stroke-width=\"2px\" d=\"M250,182.0 C250,2.0 590.0,2.0 590.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f1d22e687504be6aab936e69b8ac3aa-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M590.0,184.0 L598.0,172.0 582.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f1d22e687504be6aab936e69b8ac3aa-0-6\" stroke-width=\"2px\" d=\"M700,182.0 C700,92.0 850.0,92.0 850.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f1d22e687504be6aab936e69b8ac3aa-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M700,184.0 L692,172.0 708,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f1d22e687504be6aab936e69b8ac3aa-0-7\" stroke-width=\"2px\" d=\"M790,182.0 C790,137.0 845.0,137.0 845.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f1d22e687504be6aab936e69b8ac3aa-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M790,184.0 L782,172.0 798,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f1d22e687504be6aab936e69b8ac3aa-0-8\" stroke-width=\"2px\" d=\"M610,182.0 C610,47.0 855.0,47.0 855.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f1d22e687504be6aab936e69b8ac3aa-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M855.0,184.0 L863.0,172.0 847.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(u'Huawei to build a Istanbul factory for $4 million.')\n",
    "\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 90,\"color\":\"red\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument options lets us specify a dictionary of settings to customize the layout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entity visualizer, *ent*, highlights named entities and their labels in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Over \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the last quarter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amazon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " sold \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    nearly 30 thousand\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " books for a profit of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $5 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(u'Over the last quarter Amazon sold nearly 30 thousand books for a profit of $5 million.')\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want, we can filter entities with *options* parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"\"\"With a market capitalization of 2.25 trillion U.S. dollars as of April 2021, \n",
    "Apple was the world’s largest company in 2021. Rounding out the top five were some of the \n",
    "world’s most recognizable brands: Microsoft, Saudi Arabian Oil Company (Saudi Aramco), Amazon, \n",
    "and Google’s parent company Alphabet. Saudi Aramco led the ranking of the world's most profitable \n",
    "companies in 2019, with a net income of 88.21 billion U.S. dollars.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">With a market capitalization of 2.25 trillion U.S. dollars as of April 2021, </br>Apple was the world’s largest company in 2021. </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Rounding out the top five were some of the </br>world’s most recognizable brands: \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Microsoft\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", Saudi Arabian Oil Company (Saudi Aramco), \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amazon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", </br>and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "’s parent company Alphabet. </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Saudi Aramco\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " led the ranking of the world's most profitable </br>companies in 2019, with a net income of 88.21 billion U.S. dollars.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"></br></div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    displacy.render(sent, style='ent', jupyter=True,options={\"ents\":[\"ORG\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rendering several large documents on one page can easily become confusing. To add a headline to each visualization, we can add a title to its *user_data*. User data is never touched or modified by spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'Huawei to build a Istanbul factory for $4 million.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><h2 style=\"margin: 0\">News</h2>\n",
       "\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Huawei\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " to build a \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Istanbul\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " factory for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $4 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc.user_data[\"title\"] = \"News\"\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Upcoming Tutorials**\n",
    "\n",
    "- Text Feature Engineering \n",
    "\n",
    "- Bag of Words\n",
    "\n",
    "- Text Visualisation : Bar Plot\n",
    "\n",
    "- Text Visualisation : Frequency Visualisation\n",
    "\n",
    "- Text Visualisation : WordCloud\n",
    "\n",
    "- Transformers, Encoders and Decoders\n",
    "\n",
    "- Different Models : Bert, HuggingFace, StanfordNLP, NLTK, LSTM etc.\n",
    "\n",
    "- Sentiment Analysis with Logistic Regression\n",
    "\n",
    "- Sentiment Analysis with Naive Bayes\n",
    "\n",
    "- Vector Space Models\n",
    "\n",
    "- Neural Machine Translation\n",
    "\n",
    "- Text Summarization\n",
    "\n",
    "- Classification with Bert\n",
    "\n",
    "- Working with PDF Files\n",
    "\n",
    "***\n",
    "\n",
    "**Future Plans**\n",
    "\n",
    "- Regex\n",
    "\n",
    "- Turkish NLP\n",
    "\n",
    "- Spacy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
